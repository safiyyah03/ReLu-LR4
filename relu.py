# -*- coding: utf-8 -*-
"""ReLu.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bh8vHa4fe-8q4eUxqbiRpaal5Pnpcj09
"""

import numpy as np
import matplotlib.pyplot as plt
import sys

# Prevent multiple executions
if 'already_ran' in globals():
    sys.exit()

already_ran = True

plt.close('all')

x = np.linspace(-10, 10, 400)
y = np.maximum(0, x)

fig = plt.figure()
plt.plot(x, y)
plt.xlabel("Input")
plt.ylabel("Output")
plt.title("ReLU Activation Function")

plt.show()